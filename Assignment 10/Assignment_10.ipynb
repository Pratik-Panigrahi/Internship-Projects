{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\PRATIK\n",
      "[nltk_data]     PANIGRAHI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=pd.read_csv(r'C:\\Users\\PRATIK PANIGRAHI\\Desktop\\NLP CLASSES\\assignment 8\\NLP Projects to do\\Project_3\\Corona_NLP_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44953</td>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>44955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>44957</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>44958</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>Do you remember the last time you paid $2.99 a...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>44959</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>Voting in the age of #coronavirus = hand sanit...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>44960</td>\n",
       "      <td>Geneva, Switzerland</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>@DrTedros \"We cant stop #COVID19 without prot...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>44961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04-03-2020</td>\n",
       "      <td>HI TWITTER! I am a pharmacist. I sell hand san...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>44962</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>04-03-2020</td>\n",
       "      <td>Anyone been in a supermarket over the last few...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>44963</td>\n",
       "      <td>Boksburg, South Africa</td>\n",
       "      <td>04-03-2020</td>\n",
       "      <td>Best quality couches at unbelievably low price...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>44964</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>04-03-2020</td>\n",
       "      <td>Beware of counterfeits trying to sell fake mas...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>44965</td>\n",
       "      <td>USA, PA</td>\n",
       "      <td>04-03-2020</td>\n",
       "      <td>Panic food buying in Germany due to #coronavir...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>44966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04-03-2020</td>\n",
       "      <td>#Covid_19 Went to the Grocery Store, turns out...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>44967</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>04-03-2020</td>\n",
       "      <td>While we were busy watching election returns a...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>44968</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>04-03-2020</td>\n",
       "      <td>#AirSewa \\r\\r\\n\\r\\r\\n@flyspicejet is not provi...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>44969</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>05-03-2020</td>\n",
       "      <td>What Precautionary measures have you all taken...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>44970</td>\n",
       "      <td>Toronto, Ontario</td>\n",
       "      <td>05-03-2020</td>\n",
       "      <td>When youre stockpiling food &amp;amp; other suppl...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>44971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05-03-2020</td>\n",
       "      <td>That's about a week from now. A bit optimistic...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>44972</td>\n",
       "      <td>Tallahassee</td>\n",
       "      <td>05-03-2020</td>\n",
       "      <td>Studies show the #coronavirus like #COVID19 ca...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>44973</td>\n",
       "      <td>Malta</td>\n",
       "      <td>05-03-2020</td>\n",
       "      <td>#CoronaVirus #COVID_19 People are starting to ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>44974</td>\n",
       "      <td>Texas, USA</td>\n",
       "      <td>05-03-2020</td>\n",
       "      <td>For those of you that think credit/debit is ju...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>44975</td>\n",
       "      <td>Florida, USA</td>\n",
       "      <td>06-03-2020</td>\n",
       "      <td>Control over stocks and gold is lost...gold co...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>44976</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>06-03-2020</td>\n",
       "      <td>Sellers are cashing on your panic to sell face...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>44977</td>\n",
       "      <td>Canada</td>\n",
       "      <td>07-03-2020</td>\n",
       "      <td>Canada's oilpatch is bracing for the impact of...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>44978</td>\n",
       "      <td>Wayne County, MI</td>\n",
       "      <td>07-03-2020</td>\n",
       "      <td>Alright yall I hope our Melanin prevails but ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>44979</td>\n",
       "      <td>Bondi Beach, Sydney</td>\n",
       "      <td>07-03-2020</td>\n",
       "      <td>When your usual grocery shopping @woolworths t...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>44980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07-03-2020</td>\n",
       "      <td>Coronavirus panic spreads: Costco is pulling f...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>44981</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07-03-2020</td>\n",
       "      <td>While you are stocking up, waiting for quarant...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>44982</td>\n",
       "      <td>United States</td>\n",
       "      <td>07-03-2020</td>\n",
       "      <td>#Coronavirus is \"an exposure of all the holes ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3768</th>\n",
       "      <td>3769</td>\n",
       "      <td>48721</td>\n",
       "      <td>Montreal, Canada</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Panic: Toilet Paper Is the People...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>3770</td>\n",
       "      <td>48722</td>\n",
       "      <td>Marietta, GA</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Due to the Corona Virus (Covid -19) we have ma...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>3771</td>\n",
       "      <td>48723</td>\n",
       "      <td>West Virginia, USA</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>PSA: Stop panicking about COVID-19, you don't ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>3772</td>\n",
       "      <td>48724</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>When I'm stopped by George at the grocery stor...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>3773</td>\n",
       "      <td>48725</td>\n",
       "      <td>Louisville, KY</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Due to ongoing public health and safety concer...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>3774</td>\n",
       "      <td>48726</td>\n",
       "      <td>Köln</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Rewe Mitarbeiter: Nur eine Packung Klopapier ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td>3775</td>\n",
       "      <td>48727</td>\n",
       "      <td>Living takes my breath away?</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>The financial &amp;amp; economic impact of COVID-1...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3775</th>\n",
       "      <td>3776</td>\n",
       "      <td>48728</td>\n",
       "      <td>Southern Maine</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>At the store today, cat food was in low supply...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>3777</td>\n",
       "      <td>48729</td>\n",
       "      <td>Toronto, Ontario, Canada</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>The GLB Retail Store / Taproom is CLOSED today...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>3778</td>\n",
       "      <td>48730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Waiting in a line in 1 meter distance from eac...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>3779</td>\n",
       "      <td>48731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>In light of the ongoing and rapidly evolving C...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>3780</td>\n",
       "      <td>48732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Stuck inside?  How about getting some reading ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>3781</td>\n",
       "      <td>48733</td>\n",
       "      <td>Kansas, USA</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@GovLauraKelly PLEASE CLOSE ALL RETAIL that is...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3781</th>\n",
       "      <td>3782</td>\n",
       "      <td>48734</td>\n",
       "      <td>Groningen, Nederland</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@writing4poker @RealKidPoker I can only wish y...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3782</th>\n",
       "      <td>3783</td>\n",
       "      <td>48735</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Most brands feel compelled to address COVID-19...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3783</th>\n",
       "      <td>3784</td>\n",
       "      <td>48736</td>\n",
       "      <td>Texas</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Amazon delivery infrastructure strained as COV...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3784</th>\n",
       "      <td>3785</td>\n",
       "      <td>48737</td>\n",
       "      <td>Paraparaumu Beach</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Just been through K?piti New World which is bu...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3785</th>\n",
       "      <td>3786</td>\n",
       "      <td>48738</td>\n",
       "      <td>People's Republic of China</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>At this crucial time when non Muslims are givi...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3786</th>\n",
       "      <td>3787</td>\n",
       "      <td>48739</td>\n",
       "      <td>Flemington, New Jersey</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>We've noticed a shift in consumer #research in...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3787</th>\n",
       "      <td>3788</td>\n",
       "      <td>48740</td>\n",
       "      <td>Manchester, England</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Its funny seeing all these people fight and pa...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3788</th>\n",
       "      <td>3789</td>\n",
       "      <td>48741</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>You never eaten the pigs cat dog or food from ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3789</th>\n",
       "      <td>3790</td>\n",
       "      <td>48742</td>\n",
       "      <td>California, USA</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@calebmealer @thebradfordfile @realDonaldTrump...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3790</th>\n",
       "      <td>3791</td>\n",
       "      <td>48743</td>\n",
       "      <td>Cincinnati, Ohio</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Even though the Law Library is closed, ALL sub...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3791</th>\n",
       "      <td>3792</td>\n",
       "      <td>48744</td>\n",
       "      <td>Washington D.C.</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>With Gov Hogan's announcement that all bars, r...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3792</th>\n",
       "      <td>3793</td>\n",
       "      <td>48745</td>\n",
       "      <td>Washington D.C.</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@RicePolitics @MDCounties Craig, will you call...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>3794</td>\n",
       "      <td>48746</td>\n",
       "      <td>Israel ??</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Meanwhile In A Supermarket in Israel -- People...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3794</th>\n",
       "      <td>3795</td>\n",
       "      <td>48747</td>\n",
       "      <td>Farmington, NM</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Did you panic buy a lot of non-perishable item...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>3796</td>\n",
       "      <td>48748</td>\n",
       "      <td>Haverford, PA</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Asst Prof of Economics @cconces was on @NBCPhi...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3796</th>\n",
       "      <td>3797</td>\n",
       "      <td>48749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Gov need to do somethings instead of biar je r...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>3798</td>\n",
       "      <td>48750</td>\n",
       "      <td>Arlington, Virginia</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>I and @ForestandPaper members are committed to...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3798 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      UserName  ScreenName                      Location     TweetAt  \\\n",
       "0            1       44953                           NYC  02-03-2020   \n",
       "1            2       44954                   Seattle, WA  02-03-2020   \n",
       "2            3       44955                           NaN  02-03-2020   \n",
       "3            4       44956                   Chicagoland  02-03-2020   \n",
       "4            5       44957           Melbourne, Victoria  03-03-2020   \n",
       "5            6       44958                   Los Angeles  03-03-2020   \n",
       "6            7       44959                           NaN  03-03-2020   \n",
       "7            8       44960           Geneva, Switzerland  03-03-2020   \n",
       "8            9       44961                           NaN  04-03-2020   \n",
       "9           10       44962               Dublin, Ireland  04-03-2020   \n",
       "10          11       44963        Boksburg, South Africa  04-03-2020   \n",
       "11          12       44964                     New Delhi  04-03-2020   \n",
       "12          13       44965                       USA, PA  04-03-2020   \n",
       "13          14       44966                           NaN  04-03-2020   \n",
       "14          15       44967                Washington, DC  04-03-2020   \n",
       "15          16       44968                    Bengaluru   04-03-2020   \n",
       "16          17       44969                        Mumbai  05-03-2020   \n",
       "17          18       44970              Toronto, Ontario  05-03-2020   \n",
       "18          19       44971                           NaN  05-03-2020   \n",
       "19          20       44972                   Tallahassee  05-03-2020   \n",
       "20          21       44973                         Malta  05-03-2020   \n",
       "21          22       44974                    Texas, USA  05-03-2020   \n",
       "22          23       44975                  Florida, USA  06-03-2020   \n",
       "23          24       44976                    Chandigarh  06-03-2020   \n",
       "24          25       44977                        Canada  07-03-2020   \n",
       "25          26       44978              Wayne County, MI  07-03-2020   \n",
       "26          27       44979           Bondi Beach, Sydney  07-03-2020   \n",
       "27          28       44980                           NaN  07-03-2020   \n",
       "28          29       44981                           NaN  07-03-2020   \n",
       "29          30       44982                 United States  07-03-2020   \n",
       "...        ...         ...                           ...         ...   \n",
       "3768      3769       48721              Montreal, Canada  16-03-2020   \n",
       "3769      3770       48722                  Marietta, GA  16-03-2020   \n",
       "3770      3771       48723            West Virginia, USA  16-03-2020   \n",
       "3771      3772       48724                   Houston, TX  16-03-2020   \n",
       "3772      3773       48725                Louisville, KY  16-03-2020   \n",
       "3773      3774       48726                          Köln  16-03-2020   \n",
       "3774      3775       48727  Living takes my breath away?  16-03-2020   \n",
       "3775      3776       48728                Southern Maine  16-03-2020   \n",
       "3776      3777       48729      Toronto, Ontario, Canada  16-03-2020   \n",
       "3777      3778       48730                           NaN  16-03-2020   \n",
       "3778      3779       48731                           NaN  16-03-2020   \n",
       "3779      3780       48732                           NaN  16-03-2020   \n",
       "3780      3781       48733                   Kansas, USA  16-03-2020   \n",
       "3781      3782       48734          Groningen, Nederland  16-03-2020   \n",
       "3782      3783       48735                       Toronto  16-03-2020   \n",
       "3783      3784       48736                         Texas  16-03-2020   \n",
       "3784      3785       48737             Paraparaumu Beach  16-03-2020   \n",
       "3785      3786       48738    People's Republic of China  16-03-2020   \n",
       "3786      3787       48739        Flemington, New Jersey  16-03-2020   \n",
       "3787      3788       48740           Manchester, England  16-03-2020   \n",
       "3788      3789       48741                      Pakistan  16-03-2020   \n",
       "3789      3790       48742               California, USA  16-03-2020   \n",
       "3790      3791       48743              Cincinnati, Ohio  16-03-2020   \n",
       "3791      3792       48744               Washington D.C.  16-03-2020   \n",
       "3792      3793       48745               Washington D.C.  16-03-2020   \n",
       "3793      3794       48746                     Israel ??  16-03-2020   \n",
       "3794      3795       48747                Farmington, NM  16-03-2020   \n",
       "3795      3796       48748                 Haverford, PA  16-03-2020   \n",
       "3796      3797       48749                           NaN  16-03-2020   \n",
       "3797      3798       48750           Arlington, Virginia  16-03-2020   \n",
       "\n",
       "                                          OriginalTweet           Sentiment  \n",
       "0     TRENDING: New Yorkers encounter empty supermar...  Extremely Negative  \n",
       "1     When I couldn't find hand sanitizer at Fred Me...            Positive  \n",
       "2     Find out how you can protect yourself and love...  Extremely Positive  \n",
       "3     #Panic buying hits #NewYork City as anxious sh...            Negative  \n",
       "4     #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral  \n",
       "5     Do you remember the last time you paid $2.99 a...             Neutral  \n",
       "6     Voting in the age of #coronavirus = hand sanit...            Positive  \n",
       "7     @DrTedros \"We cant stop #COVID19 without prot...             Neutral  \n",
       "8     HI TWITTER! I am a pharmacist. I sell hand san...  Extremely Negative  \n",
       "9     Anyone been in a supermarket over the last few...  Extremely Positive  \n",
       "10    Best quality couches at unbelievably low price...            Positive  \n",
       "11    Beware of counterfeits trying to sell fake mas...  Extremely Negative  \n",
       "12    Panic food buying in Germany due to #coronavir...  Extremely Negative  \n",
       "13    #Covid_19 Went to the Grocery Store, turns out...  Extremely Positive  \n",
       "14    While we were busy watching election returns a...            Positive  \n",
       "15    #AirSewa \\r\\r\\n\\r\\r\\n@flyspicejet is not provi...  Extremely Negative  \n",
       "16    What Precautionary measures have you all taken...  Extremely Positive  \n",
       "17    When youre stockpiling food &amp; other suppl...             Neutral  \n",
       "18    That's about a week from now. A bit optimistic...            Positive  \n",
       "19    Studies show the #coronavirus like #COVID19 ca...  Extremely Positive  \n",
       "20    #CoronaVirus #COVID_19 People are starting to ...            Negative  \n",
       "21    For those of you that think credit/debit is ju...  Extremely Positive  \n",
       "22    Control over stocks and gold is lost...gold co...  Extremely Positive  \n",
       "23    Sellers are cashing on your panic to sell face...  Extremely Positive  \n",
       "24    Canada's oilpatch is bracing for the impact of...  Extremely Negative  \n",
       "25    Alright yall I hope our Melanin prevails but ...            Positive  \n",
       "26    When your usual grocery shopping @woolworths t...            Negative  \n",
       "27    Coronavirus panic spreads: Costco is pulling f...            Negative  \n",
       "28    While you are stocking up, waiting for quarant...            Negative  \n",
       "29    #Coronavirus is \"an exposure of all the holes ...            Positive  \n",
       "...                                                 ...                 ...  \n",
       "3768  Coronavirus Panic: Toilet Paper Is the People...            Negative  \n",
       "3769  Due to the Corona Virus (Covid -19) we have ma...            Positive  \n",
       "3770  PSA: Stop panicking about COVID-19, you don't ...            Negative  \n",
       "3771  When I'm stopped by George at the grocery stor...            Negative  \n",
       "3772  Due to ongoing public health and safety concer...            Positive  \n",
       "3773  Rewe Mitarbeiter: Nur eine Packung Klopapier ...             Neutral  \n",
       "3774  The financial &amp; economic impact of COVID-1...             Neutral  \n",
       "3775  At the store today, cat food was in low supply...  Extremely Negative  \n",
       "3776  The GLB Retail Store / Taproom is CLOSED today...  Extremely Positive  \n",
       "3777  Waiting in a line in 1 meter distance from eac...             Neutral  \n",
       "3778  In light of the ongoing and rapidly evolving C...             Neutral  \n",
       "3779  Stuck inside?  How about getting some reading ...            Positive  \n",
       "3780  @GovLauraKelly PLEASE CLOSE ALL RETAIL that is...            Positive  \n",
       "3781  @writing4poker @RealKidPoker I can only wish y...  Extremely Positive  \n",
       "3782  Most brands feel compelled to address COVID-19...  Extremely Positive  \n",
       "3783  Amazon delivery infrastructure strained as COV...            Negative  \n",
       "3784  Just been through K?piti New World which is bu...            Positive  \n",
       "3785  At this crucial time when non Muslims are givi...  Extremely Positive  \n",
       "3786  We've noticed a shift in consumer #research in...  Extremely Positive  \n",
       "3787  Its funny seeing all these people fight and pa...  Extremely Negative  \n",
       "3788  You never eaten the pigs cat dog or food from ...             Neutral  \n",
       "3789  @calebmealer @thebradfordfile @realDonaldTrump...  Extremely Positive  \n",
       "3790  Even though the Law Library is closed, ALL sub...            Positive  \n",
       "3791  With Gov Hogan's announcement that all bars, r...  Extremely Negative  \n",
       "3792  @RicePolitics @MDCounties Craig, will you call...            Negative  \n",
       "3793  Meanwhile In A Supermarket in Israel -- People...            Positive  \n",
       "3794  Did you panic buy a lot of non-perishable item...            Negative  \n",
       "3795  Asst Prof of Economics @cconces was on @NBCPhi...             Neutral  \n",
       "3796  Gov need to do somethings instead of biar je r...  Extremely Negative  \n",
       "3797  I and @ForestandPaper members are committed to...  Extremely Positive  \n",
       "\n",
       "[3798 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=data[['OriginalTweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRATIK PANIGRAHI\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "text['index'] = text.index\n",
    "documents = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet  index\n",
       "0  TRENDING: New Yorkers encounter empty supermar...      0\n",
       "1  When I couldn't find hand sanitizer at Fred Me...      1\n",
       "2  Find out how you can protect yourself and love...      2\n",
       "3  #Panic buying hits #NewYork City as anxious sh...      3\n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...      4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "water\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('watering', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jwelled</td>\n",
       "      <td>jwell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3        jwelled   jwell\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'jwelled', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['When', 'I', \"couldn't\", 'find', 'hand', 'sanitizer', 'at', 'Fred', 'Meyer,', 'I', 'turned', 'to', '#Amazon.', 'But', '$114.97', 'for', 'a', '2', 'pack', 'of', 'Purell??!!Check', 'out', 'how', '', '#coronavirus', 'concerns', 'are', 'driving', 'up', 'prices.', 'https://t.co/ygbipBflMY']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['couldn', 'hand', 'sanit', 'fred', 'meyer', 'turn', 'amazon', 'pack', 'purel', 'check', 'coronavirus', 'concern', 'drive', 'price', 'https', 'ygbipbflmi']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 1].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = documents['OriginalTweet'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [trend, yorker, encount, supermarket, shelv, p...\n",
       "1       [couldn, hand, sanit, fred, meyer, turn, amazo...\n",
       "2                       [protect, love, one, coronavirus]\n",
       "3       [panic, buy, hit, newyork, citi, anxious, shop...\n",
       "4       [toiletpap, dunnypap, coronavirus, covid_, new...\n",
       "5       [rememb, time, pay, gallon, regular, angel, pr...\n",
       "6       [vote, coronavirus, hand, sanit, supertuesday,...\n",
       "7       [drtedro, stop, covid, protect, healthwork, pr...\n",
       "8       [twitter, pharmacist, sell, hand, sanit, live,...\n",
       "9       [supermarket, day, go, normal, shop, night, si...\n",
       "10      [best, qualiti, couch, unbeliev, price, avail,...\n",
       "11      [bewar, counterfeit, tri, sell, fake, mask, ch...\n",
       "12      [panic, food, buy, germani, coronavirus, begin...\n",
       "13      [covid_, go, groceri, store, turn, clean, supp...\n",
       "14      [busi, watch, elect, return, brace, covid, out...\n",
       "15      [airsewa, flyspicejet, provid, webchecin, cust...\n",
       "16      [precautionari, measur, take, respect, restaur...\n",
       "17      [stockpil, food, suppli, extra, local, food, b...\n",
       "18      [week, optimist, probabl, month, suppli, chain...\n",
       "19      [studi, coronavirus, like, covid, live, day, h...\n",
       "20      [coronavirus, covid_, peopl, start, frighten, ...\n",
       "21      [think, credit, debit, good, bitcoin, come, co...\n",
       "22      [control, stock, gold, lose, gold, come, nice,...\n",
       "23      [seller, cash, panic, sell, facemask, sanit, s...\n",
       "24      [canada, oilpatch, brace, impact, plung, crude...\n",
       "25      [alright, hope, melanin, prevail, case, take, ...\n",
       "26      [usual, groceri, shop, woolworth, turn, toilet...\n",
       "27      [coronavirus, panic, spread, costco, pull, fre...\n",
       "28      [stock, wait, quarantin, forget, donat, food, ...\n",
       "29      [coronavirus, exposur, hole, social, safeti, s...\n",
       "                              ...                        \n",
       "3768    [coronavirus, panic, toilet, paper, peopl, vac...\n",
       "3769    [corona, virus, covid, decis, like, compani, c...\n",
       "3770    [stop, panic, covid, need, shelter, outsid, wo...\n",
       "3771    [stop, georg, groceri, store, cough, georg, co...\n",
       "3772    [ongo, public, health, safeti, concern, goodwi...\n",
       "3773    [rew, mitarbeit, ein, packung, klopapi, nase, ...\n",
       "3774    [financi, econom, impact, covid, closur, cance...\n",
       "3775    [store, today, food, suppli, thought, peopl, p...\n",
       "3776    [retail, store, taproom, close, today, base, r...\n",
       "3777      [wait, line, meter, distanc, supermarket, time]\n",
       "3778    [light, ongo, rapid, evolv, covid, pandem, dec...\n",
       "3779    [stick, insid, get, read, time, mind, coronavi...\n",
       "3780    [govlaurakelli, close, retail, pharmaci, groce...\n",
       "3781    [write, poker, realkidpok, wish, best, luck, n...\n",
       "3782    [brand, feel, compel, address, covid, consum, ...\n",
       "3783    [amazon, deliveri, infrastructur, strain, covi...\n",
       "3784    [piti, world, busi, staff, covid, case, wellin...\n",
       "3785    [crucial, time, muslim, give, chariti, medic, ...\n",
       "3786    [notic, shift, consum, research, respons, covi...\n",
       "3787    [funni, see, peopl, fight, panic, food, peopl,...\n",
       "3788    [eat, pig, food, intern, fast, food, chain, ta...\n",
       "3789    [calebmeal, thebradfordfil, realdonaldtrump, t...\n",
       "3790    [librari, close, subscrib, access, ebook, ebsc...\n",
       "3791    [hogan, announc, bar, restaur, gym, close, tod...\n",
       "3792    [ricepolit, mdcounti, craig, general, assembl,...\n",
       "3793    [supermarket, israel, peopl, danc, sing, stay,...\n",
       "3794    [panic, perish, item, echo, need, food, donat,...\n",
       "3795    [asst, prof, econom, cconc, nbcphiladelphia, t...\n",
       "3796    [need, someth, instead, biar, rakyat, assum, l...\n",
       "3797    [forestandpap, member, commit, safeti, employe...\n",
       "Name: OriginalTweet, Length: 3798, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 brooklyn\n",
      "1 coronavirus\n",
      "2 encount\n",
      "3 fear\n",
      "4 foodkick\n",
      "5 grocer\n",
      "6 https\n",
      "7 ivmkmsqdt\n",
      "8 maxdeliveri\n",
      "9 onlin\n",
      "10 pcrlwh\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(636 unique tokens: ['fear', 'pictur', 'sell', 'shelv', 'shopper']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_text]\n",
    "bow_corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 5 (\"amazon\") appears 1 time.\n",
      "Word 6 (\"check\") appears 1 time.\n",
      "Word 7 (\"concern\") appears 1 time.\n",
      "Word 8 (\"couldn\") appears 1 time.\n",
      "Word 9 (\"drive\") appears 1 time.\n",
      "Word 10 (\"hand\") appears 1 time.\n",
      "Word 11 (\"pack\") appears 1 time.\n",
      "Word 12 (\"sanit\") appears 1 time.\n",
      "Word 13 (\"turn\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[1]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4137498891773347),\n",
      " (1, 0.5862549915322198),\n",
      " (2, 0.3964646133100739),\n",
      " (3, 0.3118514270752582),\n",
      " (4, 0.48029221514960924)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.026*\"work\" + 0.024*\"like\" + 0.017*\"home\" + 0.015*\"stay\" + 0.015*\"water\" + 0.013*\"buy\" + 0.010*\"crazi\" + 0.010*\"retail\" + 0.010*\"thing\" + 0.010*\"want\"\n",
      "Topic: 1 \n",
      "Words: 0.025*\"retail\" + 0.020*\"close\" + 0.013*\"home\" + 0.011*\"demand\" + 0.011*\"school\" + 0.011*\"long\" + 0.010*\"week\" + 0.010*\"time\" + 0.010*\"suppli\" + 0.009*\"line\"\n",
      "Topic: 2 \n",
      "Words: 0.030*\"local\" + 0.027*\"paper\" + 0.027*\"time\" + 0.026*\"toilet\" + 0.022*\"like\" + 0.017*\"buy\" + 0.016*\"shelv\" + 0.013*\"bank\" + 0.013*\"item\" + 0.013*\"donat\"\n",
      "Topic: 3 \n",
      "Words: 0.033*\"toilet\" + 0.032*\"paper\" + 0.021*\"buy\" + 0.018*\"hand\" + 0.016*\"pandem\" + 0.015*\"mask\" + 0.013*\"today\" + 0.013*\"suppli\" + 0.012*\"sanit\" + 0.011*\"retail\"\n",
      "Topic: 4 \n",
      "Words: 0.022*\"demand\" + 0.018*\"suppli\" + 0.013*\"servic\" + 0.012*\"work\" + 0.011*\"market\" + 0.011*\"help\" + 0.011*\"drop\" + 0.011*\"increas\" + 0.010*\"think\" + 0.010*\"coronaoutbreak\"\n",
      "Topic: 5 \n",
      "Words: 0.020*\"help\" + 0.018*\"buy\" + 0.018*\"time\" + 0.018*\"come\" + 0.016*\"coronaoutbreak\" + 0.016*\"coronapocalyps\" + 0.013*\"home\" + 0.008*\"suppli\" + 0.008*\"work\" + 0.008*\"lockdown\"\n",
      "Topic: 6 \n",
      "Words: 0.021*\"consum\" + 0.020*\"retail\" + 0.015*\"work\" + 0.015*\"leav\" + 0.014*\"test\" + 0.013*\"time\" + 0.012*\"pay\" + 0.012*\"corona\" + 0.011*\"virus\" + 0.011*\"famili\"\n",
      "Topic: 7 \n",
      "Words: 0.017*\"local\" + 0.017*\"paper\" + 0.015*\"toilet\" + 0.013*\"work\" + 0.012*\"week\" + 0.012*\"leav\" + 0.011*\"test\" + 0.011*\"think\" + 0.010*\"custom\" + 0.010*\"market\"\n",
      "Topic: 8 \n",
      "Words: 0.021*\"buy\" + 0.018*\"paper\" + 0.017*\"toilet\" + 0.016*\"suppli\" + 0.014*\"stop\" + 0.013*\"like\" + 0.013*\"product\" + 0.013*\"prepar\" + 0.012*\"week\" + 0.011*\"worri\"\n",
      "Topic: 9 \n",
      "Words: 0.030*\"hand\" + 0.019*\"help\" + 0.016*\"like\" + 0.014*\"sanit\" + 0.013*\"toilet\" + 0.013*\"local\" + 0.013*\"paper\" + 0.011*\"countri\" + 0.010*\"know\" + 0.010*\"elder\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.014*\"local\" + 0.013*\"hand\" + 0.012*\"like\" + 0.012*\"today\" + 0.010*\"sanit\" + 0.009*\"paper\" + 0.008*\"busi\" + 0.008*\"toilet\" + 0.008*\"worri\" + 0.008*\"suppli\"\n",
      "Topic: 1 Word: 0.014*\"retail\" + 0.011*\"busi\" + 0.011*\"right\" + 0.010*\"shelv\" + 0.010*\"consum\" + 0.008*\"year\" + 0.008*\"staff\" + 0.007*\"think\" + 0.007*\"want\" + 0.007*\"work\"\n",
      "Topic: 2 Word: 0.013*\"mask\" + 0.012*\"like\" + 0.011*\"help\" + 0.009*\"look\" + 0.008*\"virus\" + 0.008*\"coronaoutbreak\" + 0.008*\"face\" + 0.008*\"local\" + 0.008*\"corona\" + 0.007*\"elder\"\n",
      "Topic: 3 Word: 0.010*\"panicbuy\" + 0.009*\"coronapocalyps\" + 0.009*\"know\" + 0.009*\"care\" + 0.008*\"like\" + 0.008*\"work\" + 0.008*\"coronaoutbreak\" + 0.008*\"consum\" + 0.007*\"stay\" + 0.007*\"time\"\n",
      "Topic: 4 Word: 0.012*\"coronaoutbreak\" + 0.011*\"paper\" + 0.011*\"toilet\" + 0.009*\"suppli\" + 0.009*\"close\" + 0.009*\"quarantin\" + 0.008*\"like\" + 0.008*\"shelv\" + 0.007*\"consum\" + 0.007*\"water\"\n",
      "Topic: 5 Word: 0.012*\"paper\" + 0.012*\"toilet\" + 0.012*\"like\" + 0.011*\"buy\" + 0.011*\"come\" + 0.011*\"week\" + 0.010*\"virus\" + 0.010*\"today\" + 0.009*\"corona\" + 0.009*\"home\"\n",
      "Topic: 6 Word: 0.010*\"work\" + 0.009*\"worker\" + 0.009*\"retail\" + 0.009*\"demand\" + 0.008*\"pandem\" + 0.007*\"surviv\" + 0.007*\"thank\" + 0.007*\"time\" + 0.007*\"paper\" + 0.007*\"toilet\"\n",
      "Topic: 7 Word: 0.015*\"buy\" + 0.011*\"stop\" + 0.008*\"time\" + 0.008*\"fear\" + 0.008*\"amazon\" + 0.008*\"start\" + 0.007*\"actual\" + 0.007*\"fresh\" + 0.007*\"say\" + 0.007*\"work\"\n",
      "Topic: 8 Word: 0.011*\"leav\" + 0.010*\"think\" + 0.009*\"good\" + 0.009*\"line\" + 0.009*\"coronapocalyps\" + 0.009*\"pile\" + 0.009*\"work\" + 0.009*\"toilet\" + 0.009*\"time\" + 0.008*\"paper\"\n",
      "Topic: 9 Word: 0.010*\"time\" + 0.009*\"buy\" + 0.008*\"toilet\" + 0.008*\"paper\" + 0.008*\"day\" + 0.007*\"retail\" + 0.007*\"canada\" + 0.007*\"shelv\" + 0.007*\"local\" + 0.007*\"help\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['protect', 'love', 'one', 'coronavirus']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.774975061416626\t \n",
      "Topic: 0.020*\"help\" + 0.018*\"buy\" + 0.018*\"time\" + 0.018*\"come\" + 0.016*\"coronaoutbreak\" + 0.016*\"coronapocalyps\" + 0.013*\"home\" + 0.008*\"suppli\" + 0.008*\"work\" + 0.008*\"lockdown\"\n",
      "\n",
      "Score: 0.025005269795656204\t \n",
      "Topic: 0.030*\"hand\" + 0.019*\"help\" + 0.016*\"like\" + 0.014*\"sanit\" + 0.013*\"toilet\" + 0.013*\"local\" + 0.013*\"paper\" + 0.011*\"countri\" + 0.010*\"know\" + 0.010*\"elder\"\n",
      "\n",
      "Score: 0.0250034611672163\t \n",
      "Topic: 0.017*\"local\" + 0.017*\"paper\" + 0.015*\"toilet\" + 0.013*\"work\" + 0.012*\"week\" + 0.012*\"leav\" + 0.011*\"test\" + 0.011*\"think\" + 0.010*\"custom\" + 0.010*\"market\"\n",
      "\n",
      "Score: 0.025003183633089066\t \n",
      "Topic: 0.026*\"work\" + 0.024*\"like\" + 0.017*\"home\" + 0.015*\"stay\" + 0.015*\"water\" + 0.013*\"buy\" + 0.010*\"crazi\" + 0.010*\"retail\" + 0.010*\"thing\" + 0.010*\"want\"\n",
      "\n",
      "Score: 0.025003105401992798\t \n",
      "Topic: 0.022*\"demand\" + 0.018*\"suppli\" + 0.013*\"servic\" + 0.012*\"work\" + 0.011*\"market\" + 0.011*\"help\" + 0.011*\"drop\" + 0.011*\"increas\" + 0.010*\"think\" + 0.010*\"coronaoutbreak\"\n",
      "\n",
      "Score: 0.025002552196383476\t \n",
      "Topic: 0.021*\"consum\" + 0.020*\"retail\" + 0.015*\"work\" + 0.015*\"leav\" + 0.014*\"test\" + 0.013*\"time\" + 0.012*\"pay\" + 0.012*\"corona\" + 0.011*\"virus\" + 0.011*\"famili\"\n",
      "\n",
      "Score: 0.02500236965715885\t \n",
      "Topic: 0.033*\"toilet\" + 0.032*\"paper\" + 0.021*\"buy\" + 0.018*\"hand\" + 0.016*\"pandem\" + 0.015*\"mask\" + 0.013*\"today\" + 0.013*\"suppli\" + 0.012*\"sanit\" + 0.011*\"retail\"\n",
      "\n",
      "Score: 0.025001976639032364\t \n",
      "Topic: 0.030*\"local\" + 0.027*\"paper\" + 0.027*\"time\" + 0.026*\"toilet\" + 0.022*\"like\" + 0.017*\"buy\" + 0.016*\"shelv\" + 0.013*\"bank\" + 0.013*\"item\" + 0.013*\"donat\"\n",
      "\n",
      "Score: 0.02500169537961483\t \n",
      "Topic: 0.025*\"retail\" + 0.020*\"close\" + 0.013*\"home\" + 0.011*\"demand\" + 0.011*\"school\" + 0.011*\"long\" + 0.010*\"week\" + 0.010*\"time\" + 0.010*\"suppli\" + 0.009*\"line\"\n",
      "\n",
      "Score: 0.025001337751746178\t \n",
      "Topic: 0.021*\"buy\" + 0.018*\"paper\" + 0.017*\"toilet\" + 0.016*\"suppli\" + 0.014*\"stop\" + 0.013*\"like\" + 0.013*\"product\" + 0.013*\"prepar\" + 0.012*\"week\" + 0.011*\"worri\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[2]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation by classifying sample document using LDA TF-IDF model¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7749654650688171\t \n",
      "Topic: 0.010*\"panicbuy\" + 0.009*\"coronapocalyps\" + 0.009*\"know\" + 0.009*\"care\" + 0.008*\"like\" + 0.008*\"work\" + 0.008*\"coronaoutbreak\" + 0.008*\"consum\" + 0.007*\"stay\" + 0.007*\"time\"\n",
      "\n",
      "Score: 0.02500942535698414\t \n",
      "Topic: 0.010*\"work\" + 0.009*\"worker\" + 0.009*\"retail\" + 0.009*\"demand\" + 0.008*\"pandem\" + 0.007*\"surviv\" + 0.007*\"thank\" + 0.007*\"time\" + 0.007*\"paper\" + 0.007*\"toilet\"\n",
      "\n",
      "Score: 0.02500431053340435\t \n",
      "Topic: 0.013*\"mask\" + 0.012*\"like\" + 0.011*\"help\" + 0.009*\"look\" + 0.008*\"virus\" + 0.008*\"coronaoutbreak\" + 0.008*\"face\" + 0.008*\"local\" + 0.008*\"corona\" + 0.007*\"elder\"\n",
      "\n",
      "Score: 0.02500378154218197\t \n",
      "Topic: 0.012*\"paper\" + 0.012*\"toilet\" + 0.012*\"like\" + 0.011*\"buy\" + 0.011*\"come\" + 0.011*\"week\" + 0.010*\"virus\" + 0.010*\"today\" + 0.009*\"corona\" + 0.009*\"home\"\n",
      "\n",
      "Score: 0.025003762915730476\t \n",
      "Topic: 0.015*\"buy\" + 0.011*\"stop\" + 0.008*\"time\" + 0.008*\"fear\" + 0.008*\"amazon\" + 0.008*\"start\" + 0.007*\"actual\" + 0.007*\"fresh\" + 0.007*\"say\" + 0.007*\"work\"\n",
      "\n",
      "Score: 0.025003135204315186\t \n",
      "Topic: 0.010*\"time\" + 0.009*\"buy\" + 0.008*\"toilet\" + 0.008*\"paper\" + 0.008*\"day\" + 0.007*\"retail\" + 0.007*\"canada\" + 0.007*\"shelv\" + 0.007*\"local\" + 0.007*\"help\"\n",
      "\n",
      "Score: 0.025002693757414818\t \n",
      "Topic: 0.011*\"leav\" + 0.010*\"think\" + 0.009*\"good\" + 0.009*\"line\" + 0.009*\"coronapocalyps\" + 0.009*\"pile\" + 0.009*\"work\" + 0.009*\"toilet\" + 0.009*\"time\" + 0.008*\"paper\"\n",
      "\n",
      "Score: 0.025002656504511833\t \n",
      "Topic: 0.012*\"coronaoutbreak\" + 0.011*\"paper\" + 0.011*\"toilet\" + 0.009*\"suppli\" + 0.009*\"close\" + 0.009*\"quarantin\" + 0.008*\"like\" + 0.008*\"shelv\" + 0.007*\"consum\" + 0.007*\"water\"\n",
      "\n",
      "Score: 0.02500259503722191\t \n",
      "Topic: 0.014*\"local\" + 0.013*\"hand\" + 0.012*\"like\" + 0.012*\"today\" + 0.010*\"sanit\" + 0.009*\"paper\" + 0.008*\"busi\" + 0.008*\"toilet\" + 0.008*\"worri\" + 0.008*\"suppli\"\n",
      "\n",
      "Score: 0.0250021331012249\t \n",
      "Topic: 0.014*\"retail\" + 0.011*\"busi\" + 0.011*\"right\" + 0.010*\"shelv\" + 0.010*\"consum\" + 0.008*\"year\" + 0.008*\"staff\" + 0.007*\"think\" + 0.007*\"want\" + 0.007*\"work\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[2]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.47215038537979126\t Topic: 0.021*\"buy\" + 0.018*\"paper\" + 0.017*\"toilet\" + 0.016*\"suppli\" + 0.014*\"stop\"\n",
      "Score: 0.4278135299682617\t Topic: 0.026*\"work\" + 0.024*\"like\" + 0.017*\"home\" + 0.015*\"stay\" + 0.015*\"water\"\n",
      "Score: 0.012506399303674698\t Topic: 0.020*\"help\" + 0.018*\"buy\" + 0.018*\"time\" + 0.018*\"come\" + 0.016*\"coronaoutbreak\"\n",
      "Score: 0.012505967170000076\t Topic: 0.030*\"hand\" + 0.019*\"help\" + 0.016*\"like\" + 0.014*\"sanit\" + 0.013*\"toilet\"\n",
      "Score: 0.012505476363003254\t Topic: 0.017*\"local\" + 0.017*\"paper\" + 0.015*\"toilet\" + 0.013*\"work\" + 0.012*\"week\"\n",
      "Score: 0.012504578568041325\t Topic: 0.022*\"demand\" + 0.018*\"suppli\" + 0.013*\"servic\" + 0.012*\"work\" + 0.011*\"market\"\n",
      "Score: 0.012503928504884243\t Topic: 0.030*\"local\" + 0.027*\"paper\" + 0.027*\"time\" + 0.026*\"toilet\" + 0.022*\"like\"\n",
      "Score: 0.012503789737820625\t Topic: 0.025*\"retail\" + 0.020*\"close\" + 0.013*\"home\" + 0.011*\"demand\" + 0.011*\"school\"\n",
      "Score: 0.012502999976277351\t Topic: 0.033*\"toilet\" + 0.032*\"paper\" + 0.021*\"buy\" + 0.018*\"hand\" + 0.016*\"pandem\"\n",
      "Score: 0.01250296551734209\t Topic: 0.021*\"consum\" + 0.020*\"retail\" + 0.015*\"work\" + 0.015*\"leav\" + 0.014*\"test\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document='LetÂ’s ALL look after the less capable in our village and ensure they stay healthy. Bringing shopping to their doors, help with online '\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
